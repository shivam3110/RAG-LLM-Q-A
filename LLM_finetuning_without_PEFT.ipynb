{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMQJZs-35Tal",
        "outputId": "b851f8f6-c766-4946-8bb3-89b3a8170caa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.3/205.3 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q nltk\n",
        "!pip install -q datasets\n",
        "!pip install -q transformers[torch]\n",
        "!pip install -q tokenizers\n",
        "!pip install -q evaluate\n",
        "!pip install -q rouge_score\n",
        "!pip install -q sentencepiece\n",
        "!pip install -q huggingface_hub\n",
        "!pip install -q pypdf\n",
        "!pip install -qqq bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdyGZZNEx_jG"
      },
      "outputs": [],
      "source": [
        "# Standard library imports\n",
        "import numpy as np\n",
        "\n",
        "# External library imports\n",
        "import nltk\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "from transformers import T5Tokenizer, DataCollatorForSeq2Seq, T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
        "\n",
        "# Local module imports\n",
        "from evaluate import evaluate\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, TextSplitter, CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_VPhVxEAjQE"
      },
      "source": [
        "# Pretrained model for finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMrDCSQwYkrE",
        "outputId": "84f952df-db11-43e9-a01c-ed943c8477df"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "from transformers import T5Tokenizer, DataCollatorForSeq2Seq,T5ForConditionalGeneration\n",
        "# Load the tokenizer, model, and data collator\n",
        "\n",
        "# MODELS:\n",
        "\n",
        "# 1. \"google/flan-t5-small\"\n",
        "# 2. \"google/flan-t5-base\"\n",
        "# 3. \"google/flan-t5-large\"\n",
        "\n",
        "MODEL_NAME = \"google/flan-t5-base\"\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
        "model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME,device_map=\"auto\")\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMNbDognVBxY"
      },
      "source": [
        "# **Dataset preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TnRDh0WTVBxf"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "clean_df = pd.read_json('/content/cleanquest.json')\n",
        "\n",
        "train = clean_df.head(1000)\n",
        "test = clean_df.tail(452)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train)\n",
        "val_dataset = Dataset.from_pandas(test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaAEEGVTAQO0"
      },
      "source": [
        "## Preparing Knowledge base using FAISS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZOxcUDEVBxg"
      },
      "outputs": [],
      "source": [
        "\n",
        "list_of_documents=[]\n",
        "\n",
        "for clean_exp in train.clean_explanation.values:\n",
        "  text_splitter = CharacterTextSplitter(separator='\\n',chunk_size=256,chunk_overlap=16)\n",
        "  list_of_documents.extend(text_splitter.split_documents([Document(page_content=clean_exp)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHn5_KhTVBxh"
      },
      "outputs": [],
      "source": [
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "#modelPath = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device':'cpu'}\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI9kJu4WVBxh"
      },
      "outputs": [],
      "source": [
        "db = FAISS.from_documents(list_of_documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx51iEL0VCWB"
      },
      "source": [
        "## Preparing data to feed into the finetuning process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIZ5nvpdVBxf"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(question, answer, max_length=512):\n",
        "  inputs = tokenizer(\n",
        "  question,\n",
        "  max_length=max_length,\n",
        "  truncation='only_second',\n",
        "  padding='max_length',\n",
        "  return_attention_mask=True,\n",
        "  add_special_tokens=True,\n",
        "  return_tensors='pt'\n",
        "  )\n",
        "  input_ids = inputs['input_ids'].squeeze()\n",
        "  attention_mask = inputs['attention_mask'].squeeze()\n",
        "\n",
        "  labels = tokenizer(text_target=answer, max_length=max_length, padding='max_length', truncation=True)\n",
        "\n",
        "  # replace all tokenizer.pad_token_id in the labels by -100 \n",
        "  labels[\"input_ids\"] = [(l if l != tokenizer.pad_token_id else -100) for l in labels[\"input_ids\"]]\n",
        "  inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  return {\n",
        "      'input_ids': input_ids,\n",
        "      'attention_mask': attention_mask,\n",
        "      'labels': inputs[\"labels\"]\n",
        "\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuiOGNxlVBxh",
        "outputId": "bf42e3dc-3bec-4fbd-a584-21f0d53a82c4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [00:22<00:00, 43.97it/s]\n",
            "100%|██████████| 452/452 [00:10<00:00, 44.04it/s]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" preparing the data in such a way that the input text is [context + question + options]\n",
        "and the target output is the correct answer.Context is the list of documents that are \n",
        "retrieved given the question and the options\"\"\"\n",
        "\n",
        "preprocessed_train_dataset = []\n",
        "preprocessed_val_dataset = []\n",
        "\n",
        "\n",
        "for example in tqdm(train_dataset):\n",
        "  question=example['question']\n",
        "  option1=example['answers'][0]['answer']\n",
        "  option2=example['answers'][1]['answer']\n",
        "  option3=example['answers'][2]['answer']\n",
        "  option4=example['answers'][3]['answer']\n",
        "  context=''\n",
        "  for docs in db.search(question,search_type='mmr',k=10):\n",
        "    context+=docs.page_content+'\\n'\n",
        "\n",
        "  question=\"context: \"+context+ \" Use the context to answer the following question. Answer using the context only. For the question: \"+ question +\" ,choose the correct answer from the following answers: option1) \" + option1 +\", option2) \"+ option2 +\", option3) \"+ option3 + \", option4) \"+option4\n",
        "  preprocessed_example = preprocess_data(question, example['correct_answer'])\n",
        "  preprocessed_train_dataset.append(preprocessed_example)\n",
        "\n",
        "for example in tqdm(val_dataset):\n",
        "  question=example['question']\n",
        "  option1=example['answers'][0]['answer']\n",
        "  option2=example['answers'][1]['answer']\n",
        "  option3=example['answers'][2]['answer']\n",
        "  option4=example['answers'][3]['answer']\n",
        "  context=''\n",
        "  for docs in db.search(question,search_type='mmr',k=10):\n",
        "    context+=docs.page_content+'\\n'\n",
        "  question=\"context: \"+context+ \" Use the context to answer the following question. Answer using the context only.For the question: \"+ question +\" ,choose the correct answer from the following answers: option1) \" + option1 +\", option2) \"+ option2 +\", option3) \"+ option3 + \", option4) \"+option4\n",
        "  preprocessed_example = preprocess_data(question, example['correct_answer'])\n",
        "  preprocessed_val_dataset.append(preprocessed_example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFvpasFhVBxi",
        "outputId": "02a3b633-bfc3-495a-e904-a32e178bc6cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 452\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tokenized_train_dataset = Dataset.from_dict(\n",
        "{key: [example[key] for example in preprocessed_train_dataset] for key in preprocessed_train_dataset[0].keys()})\n",
        "print(tokenized_train_dataset)\n",
        "\n",
        "tokenized_val_dataset = Dataset.from_dict(\n",
        "{key: [example[key] for example in preprocessed_val_dataset] for key in preprocessed_val_dataset[0].keys()})\n",
        "print(tokenized_val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qcawpYC0BRmr"
      },
      "source": [
        "# **Finetuning**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdJv2wh8bfEA"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"punkt\", quiet=True)\n",
        "metric = evaluate.load(\"rouge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecg4sAJDbjuh"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_preds):\n",
        "\n",
        "   preds, labels = eval_preds\n",
        "\n",
        "   # decode preds and labels\n",
        "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "   # rougeLSum expects newline after each sentence\n",
        "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "   return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_sPw6yGnArC"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jebWVPIjboaR"
      },
      "outputs": [],
      "source": [
        "# Global Parameters\n",
        "L_RATE = 15e-6  #3e-4 #15e-6,\n",
        "BATCH_SIZE = 4\n",
        "PER_DEVICE_EVAL_BATCH = 4\n",
        "WEIGHT_DECAY = 0.02\n",
        "SAVE_TOTAL_LIM = 5\n",
        "NUM_EPOCHS = 25\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir=\"./results_25\",\n",
        "   evaluation_strategy=\"epoch\",\n",
        "   learning_rate=L_RATE,\n",
        "   per_device_train_batch_size=BATCH_SIZE,\n",
        "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
        "   weight_decay=WEIGHT_DECAY,\n",
        "   save_total_limit=SAVE_TOTAL_LIM,\n",
        "   num_train_epochs=NUM_EPOCHS,\n",
        "   predict_with_generate=True,\n",
        "   push_to_hub=False,\n",
        "   save_strategy='epoch',\n",
        "   load_best_model_at_end=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFB-7HkBbrMT"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train_dataset,\n",
        "   eval_dataset=tokenized_val_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 948
        },
        "id": "E_ZFtbC8OgKx",
        "outputId": "8fb8dc40-d29c-4f7a-f933-b07e20430f0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 1:13:17, Epoch 25/25]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.167608</td>\n",
              "      <td>0.637460</td>\n",
              "      <td>0.476441</td>\n",
              "      <td>0.622417</td>\n",
              "      <td>0.624794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.220200</td>\n",
              "      <td>0.170987</td>\n",
              "      <td>0.639996</td>\n",
              "      <td>0.483059</td>\n",
              "      <td>0.626897</td>\n",
              "      <td>0.629414</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.220200</td>\n",
              "      <td>0.180074</td>\n",
              "      <td>0.637783</td>\n",
              "      <td>0.475566</td>\n",
              "      <td>0.625624</td>\n",
              "      <td>0.628006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.160800</td>\n",
              "      <td>0.186673</td>\n",
              "      <td>0.641845</td>\n",
              "      <td>0.487079</td>\n",
              "      <td>0.629645</td>\n",
              "      <td>0.632535</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.160800</td>\n",
              "      <td>0.185428</td>\n",
              "      <td>0.648075</td>\n",
              "      <td>0.490834</td>\n",
              "      <td>0.635020</td>\n",
              "      <td>0.636394</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.128400</td>\n",
              "      <td>0.182793</td>\n",
              "      <td>0.654168</td>\n",
              "      <td>0.495114</td>\n",
              "      <td>0.641290</td>\n",
              "      <td>0.642607</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.128400</td>\n",
              "      <td>0.203357</td>\n",
              "      <td>0.667616</td>\n",
              "      <td>0.513890</td>\n",
              "      <td>0.655059</td>\n",
              "      <td>0.656247</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.103000</td>\n",
              "      <td>0.212303</td>\n",
              "      <td>0.651549</td>\n",
              "      <td>0.496788</td>\n",
              "      <td>0.636881</td>\n",
              "      <td>0.637420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.103000</td>\n",
              "      <td>0.227262</td>\n",
              "      <td>0.658034</td>\n",
              "      <td>0.506613</td>\n",
              "      <td>0.646005</td>\n",
              "      <td>0.646495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.088100</td>\n",
              "      <td>0.235684</td>\n",
              "      <td>0.659679</td>\n",
              "      <td>0.514818</td>\n",
              "      <td>0.648834</td>\n",
              "      <td>0.649956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.088100</td>\n",
              "      <td>0.247861</td>\n",
              "      <td>0.653810</td>\n",
              "      <td>0.509580</td>\n",
              "      <td>0.642253</td>\n",
              "      <td>0.643010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.074700</td>\n",
              "      <td>0.253637</td>\n",
              "      <td>0.654525</td>\n",
              "      <td>0.508340</td>\n",
              "      <td>0.643750</td>\n",
              "      <td>0.644564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.074700</td>\n",
              "      <td>0.271017</td>\n",
              "      <td>0.656860</td>\n",
              "      <td>0.508571</td>\n",
              "      <td>0.643205</td>\n",
              "      <td>0.644173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.064800</td>\n",
              "      <td>0.279390</td>\n",
              "      <td>0.658263</td>\n",
              "      <td>0.511183</td>\n",
              "      <td>0.645854</td>\n",
              "      <td>0.646832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.064800</td>\n",
              "      <td>0.285986</td>\n",
              "      <td>0.656631</td>\n",
              "      <td>0.510973</td>\n",
              "      <td>0.645802</td>\n",
              "      <td>0.646445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.058900</td>\n",
              "      <td>0.291685</td>\n",
              "      <td>0.657207</td>\n",
              "      <td>0.510268</td>\n",
              "      <td>0.646017</td>\n",
              "      <td>0.645517</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.058900</td>\n",
              "      <td>0.296411</td>\n",
              "      <td>0.656519</td>\n",
              "      <td>0.505674</td>\n",
              "      <td>0.644835</td>\n",
              "      <td>0.645415</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.312487</td>\n",
              "      <td>0.651278</td>\n",
              "      <td>0.501304</td>\n",
              "      <td>0.641206</td>\n",
              "      <td>0.642165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.052000</td>\n",
              "      <td>0.325909</td>\n",
              "      <td>0.660623</td>\n",
              "      <td>0.513222</td>\n",
              "      <td>0.651158</td>\n",
              "      <td>0.651146</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.041200</td>\n",
              "      <td>0.331223</td>\n",
              "      <td>0.653099</td>\n",
              "      <td>0.505401</td>\n",
              "      <td>0.641557</td>\n",
              "      <td>0.642106</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.041200</td>\n",
              "      <td>0.329210</td>\n",
              "      <td>0.652229</td>\n",
              "      <td>0.505696</td>\n",
              "      <td>0.641017</td>\n",
              "      <td>0.641807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.048100</td>\n",
              "      <td>0.328814</td>\n",
              "      <td>0.654370</td>\n",
              "      <td>0.505421</td>\n",
              "      <td>0.643115</td>\n",
              "      <td>0.643529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.048100</td>\n",
              "      <td>0.335003</td>\n",
              "      <td>0.656515</td>\n",
              "      <td>0.508046</td>\n",
              "      <td>0.644777</td>\n",
              "      <td>0.645309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>0.335552</td>\n",
              "      <td>0.655397</td>\n",
              "      <td>0.508185</td>\n",
              "      <td>0.643927</td>\n",
              "      <td>0.644197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.043800</td>\n",
              "      <td>0.336457</td>\n",
              "      <td>0.655086</td>\n",
              "      <td>0.507822</td>\n",
              "      <td>0.643563</td>\n",
              "      <td>0.643680</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6250, training_loss=0.08841370223999023, metrics={'train_runtime': 4398.7732, 'train_samples_per_second': 5.683, 'train_steps_per_second': 1.421, 'total_flos': 2.497696310321971e+16, 'train_loss': 0.08841370223999023, 'epoch': 25.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNRFZJSsBbB6"
      },
      "source": [
        "# **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyvWxsKgKdBk",
        "outputId": "998145af-19bf-47bc-cf52-a3e91a56d4ca"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "last_checkpoint = '/content/results_25/checkpoint-6250'\n",
        "\n",
        "finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)\n",
        "finetuned_tokenizer = T5Tokenizer.from_pretrained(last_checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ej7COQvbLuZ"
      },
      "outputs": [],
      "source": [
        "question_answerer = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=finetuned_model,\n",
        "    tokenizer=finetuned_tokenizer\n",
        ")\n",
        "\n",
        "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
        "# with additional model-specific arguments (temperature and max_length)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=question_answerer,\n",
        "    model_kwargs={\"temperature\": 0, \"max_length\": 512},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyht-KvTbTee"
      },
      "outputs": [],
      "source": [
        "\n",
        "retriever = db.as_retriever(\n",
        "     search_kwargs={\"k\":10}, search_type = 'mmr'\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True, chain_type='stuff')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MoMp23bhbUY1",
        "outputId": "d39631da-2ea0-4457-c284-261e10b08a50"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 452/452 [05:31<00:00,  1.36it/s]\n"
          ]
        }
      ],
      "source": [
        "answers=[]\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(1000,1452)):\n",
        "  question=clean_df['question'].iloc[i]\n",
        "  option1=clean_df['answers'].iloc[i][0]['answer']\n",
        "  option2=clean_df['answers'].iloc[i][1]['answer']\n",
        "  option3=clean_df['answers'].iloc[i][2]['answer']\n",
        "  option4=clean_df['answers'].iloc[i][3]['answer']\n",
        "  # question=\"For the question\"+ question +\"choose the correct answer from the following answers\" + option1 +\",\"+ option2 +\",\"+ option3 + \",\"+option4\n",
        "  question=\"For the question: \"+ question +\" ,choose the correct answer from the following answers: option1) \" + option1 +\", option2) \"+ option2 +\", option3) \"+ option3 + \", option4) \"+option4\n",
        "  response = qa({\"query\": question},return_only_outputs=True)\n",
        "  #print(response['result'])\n",
        "  answers.append(response['result'].lower().replace(clean_df['question'].iloc[i].lower(),'').strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOq66CNVB_cb"
      },
      "source": [
        "# **Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdxJXTZpbheH",
        "outputId": "29884a4b-8c22-4f18-81b2-8195a4d5f0f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "164 out of 452 are correct\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "\n",
        "cnt = 0\n",
        "j = 0\n",
        "for i in range(len(answers)):\n",
        "  matches = 0\n",
        "  if answers[j].lower().strip().translate(str.maketrans('', '', string.punctuation)).replace('.','').replace('helpful','').replace('help','')==clean_df.tail(452)['correct_answer'].values[i].lower().strip().translate(str.maketrans('', '', string.punctuation)).replace('.',''):\n",
        "    cnt+=1\n",
        "    matches=1\n",
        "\n",
        "  row_data = {'predicted_answer': answers[j].replace('helpful','').replace('help',''), 'groundtruth': clean_df.tail(452)['correct_answer'].values[i], 'matches?': matches, 'options':clean_df.tail(452)['answers'].values[i]}  # Replace with your values\n",
        "  result_df = result_df.append(row_data, ignore_index=True)\n",
        "  j+=1\n",
        "print(str(cnt)+\" out of \" + str(len(answers))+\" are correct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj_gKJJnba2h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "columns = ['predicted_answer', 'groundtruth', 'matches?','options']\n",
        "result_df = pd.DataFrame(columns=columns)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqGrk09CnHip"
      },
      "outputs": [],
      "source": [
        "result_df['groundtruth'] = result_df['groundtruth'].str.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKfaa_Yjs5uD"
      },
      "outputs": [],
      "source": [
        "def rouge_l_score(reference, generated):\n",
        "    smoothing_function = SmoothingFunction().method1\n",
        "    reference_tokens = reference.split()\n",
        "    generated_tokens = generated.split()\n",
        "\n",
        "    # Compute ROUGE-L score\n",
        "    rouge_l = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_function)\n",
        "\n",
        "    return rouge_l\n",
        "\n",
        "# mapping the groundtruth string to option id using string match\n",
        "def map_to_option_id_gd(row):\n",
        "  target_answer=row['groundtruth']\n",
        "  answer_options=row['options']\n",
        "  for option in answer_options:\n",
        "      if target_answer.lower() in option['answer'].lower():\n",
        "          return int(option['id'])\n",
        "  return None\n",
        "\n",
        "#matching the predicted answer to option id by calculating rougel score between the predicted answer and \n",
        "# the options and the option with the maximum rougel score is returned as the predicted option\n",
        "def map_to_option_id_pred(row):\n",
        "  target_answer=row['predicted_answer']\n",
        "  answer_options=row['options']\n",
        "  maxrouge=0\n",
        "  id=None\n",
        "  for option in answer_options:\n",
        "      rouge_lscore=rouge_l_score(option['answer'].lower(),target_answer.lower())\n",
        "      if rouge_lscore > maxrouge:\n",
        "          maxrouge=rouge_lscore\n",
        "          id=int(option['id'])\n",
        "  return id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyS7HvEFs5uD"
      },
      "outputs": [],
      "source": [
        "result_df['result_id_groundtruth'] = result_df.apply(map_to_option_id_gd, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S0uBqgvJs5uE"
      },
      "outputs": [],
      "source": [
        "result_df['result_id_prediction']=result_df.apply(map_to_option_id_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ltQ2qmzPs5uE"
      },
      "outputs": [],
      "source": [
        "result_df['matches_ids']=result_df.result_id_groundtruth==result_df.result_id_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K49GNScYs5uE"
      },
      "outputs": [],
      "source": [
        "result_df.matches_ids.value_counts()[True]/len(result_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYznbOgJD6Gz",
        "outputId": "b8e76ee6-ef7e-472f-c77e-de5fdaadaa59"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    246\n",
              "True     206\n",
              "Name: matches_ids, dtype: int64"
            ]
          },
          "execution_count": 124,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df.matches_ids.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMjTOvVwCMFO"
      },
      "outputs": [],
      "source": [
        "206/452  #accuracy of the finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5Ti0hxen7Po"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
