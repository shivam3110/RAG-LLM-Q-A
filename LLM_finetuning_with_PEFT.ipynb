{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ShAuuHCDDkvk",
      "metadata": {
        "id": "ShAuuHCDDkvk"
      },
      "source": [
        "## Install requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "DRQ4ZrJTDkSy",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRQ4ZrJTDkSy",
        "outputId": "bcecbf01-6256-4ef2-e87b-393060a47c46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git@main"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QBdCIrizDxFw",
      "metadata": {
        "id": "QBdCIrizDxFw"
      },
      "source": [
        "## Import model and tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dd3c5acc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd3c5acc",
        "outputId": "a351eea2-641e-4ee1-f04a-924e5cd93aeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Select CUDA device index\n",
        "import os\n",
        "import torch\n",
        "#from datasets import load_dataset\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "\n",
        "\n",
        "model_name = \"google/flan-t5-base\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VwcHieQzD_dl",
      "metadata": {
        "id": "VwcHieQzD_dl"
      },
      "source": [
        "## Prepare model for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1629ebcb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1629ebcb",
        "outputId": "0b9bef38-3725-4484-f440-119790837dd0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/peft/utils/other.py:143: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from peft import prepare_model_for_int8_training\n",
        "\n",
        "model = prepare_model_for_int8_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iCpAgawAEieu",
      "metadata": {
        "id": "iCpAgawAEieu"
      },
      "source": [
        "## Load your `PeftModel`\n",
        "\n",
        "LoRA (Low-Rank Adaptators) to train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "17566ae3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17566ae3",
        "outputId": "05dfa553-360c-4f15-9993-a8f0ababdb7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 1769472 || all params: 249347328 || trainable%: 0.7096414524241463\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, get_peft_model, TaskType\n",
        "\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16, lora_alpha=32, target_modules=[\"q\", \"v\"], lora_dropout=0.05, bias=\"none\", task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HsG0x6Z7FwjZ",
      "metadata": {
        "id": "HsG0x6Z7FwjZ"
      },
      "source": [
        "## Load and process data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "q3JtaprcVBxZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "q3JtaprcVBxZ",
        "outputId": "3658eef2-8214-4a34-a1b0-e00bb02b1f14"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-62dccbe8-13b8-4924-bc95-dd79a3c49eb3\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>answers</th>\n",
              "      <th>correct_answer</th>\n",
              "      <th>explanation</th>\n",
              "      <th>explanationImageHtml</th>\n",
              "      <th>id</th>\n",
              "      <th>clean_explanation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mountain waves should be expected..</td>\n",
              "      <td>[{'id': 1, 'answer': 'directly over the mounta...</td>\n",
              "      <td>on the downwind side of the mountain range.</td>\n",
              "      <td>Refer to figure.\\nMOUNTAIN/LEE WAVES\\ndevelop ...</td>\n",
              "      <td>&lt;div class=\"mt10 fl wd100\"&gt; &lt;div class=\"fl tc ...</td>\n",
              "      <td>72880</td>\n",
              "      <td>.\\nMOUNTAIN/LEE WAVES\\ndevelop on the lee, or ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Which cloud poses the highest risk to aviation...</td>\n",
              "      <td>[{'id': 1, 'answer': 'Cumulus fractus.'}, {'id...</td>\n",
              "      <td>Cumulonimbus.</td>\n",
              "      <td>Refer to figure.\\nWhen cold air displaces warm...</td>\n",
              "      <td>&lt;div class=\"mt10 fl wd100\"&gt; &lt;div class=\"fl tc ...</td>\n",
              "      <td>96266</td>\n",
              "      <td>.\\nWhen cold air displaces warm air, it create...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>According to ICAO Annex 3 definition, \"visibil...</td>\n",
              "      <td>[{'id': 1, 'answer': '1 is correct, 2 is corre...</td>\n",
              "      <td>1 is correct, 2 is incorrect.</td>\n",
              "      <td>In effect,\\nvisibility\\nis a measure of atmosp...</td>\n",
              "      <td></td>\n",
              "      <td>96218</td>\n",
              "      <td>In effect,\\nvisibility\\nis a measure of atmosp...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>An aerodrome on the coast (Faro, Portugal) rep...</td>\n",
              "      <td>[{'id': 1, 'answer': 'Humidity reducing becaus...</td>\n",
              "      <td>A cold front crossing the area.</td>\n",
              "      <td>Refer to figure.\\nNote: This question has two ...</td>\n",
              "      <td>&lt;div class=\"mt10 fl wd100\"&gt; &lt;div class=\"fl tc ...</td>\n",
              "      <td>80606</td>\n",
              "      <td>.\\nNote: This question has two correct answers...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Which of the following statements concerning a...</td>\n",
              "      <td>[{'id': 1, 'answer': 'Air parcels that are for...</td>\n",
              "      <td>An isothermal layer is absolutely stable.</td>\n",
              "      <td>An isothermal layer is defined as a vertical c...</td>\n",
              "      <td></td>\n",
              "      <td>72780</td>\n",
              "      <td>An isothermal layer is defined as a vertical c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1447</th>\n",
              "      <td>A mountain range spans from North to South. A ...</td>\n",
              "      <td>[{'id': 1, 'answer': 'Cloud base higher than 2...</td>\n",
              "      <td>Cloud base higher than 2 000 ft and colder air...</td>\n",
              "      <td>Refer to figure.\\nThis question is a little tr...</td>\n",
              "      <td>&lt;div class=\"mt10 fl wd100\"&gt; &lt;div class=\"fl tc ...</td>\n",
              "      <td>82192</td>\n",
              "      <td>.\\nThis question is a little tricky as it is a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1448</th>\n",
              "      <td>Which type of cloud is most likely to be encou...</td>\n",
              "      <td>[{'id': 1, 'answer': 'Altocumulus'}, {'id': 2,...</td>\n",
              "      <td>Altocumulus</td>\n",
              "      <td>Refer to figure.\\nNote: This question is quite...</td>\n",
              "      <td>&lt;div class=\"mt10 fl wd100\"&gt; &lt;div class=\"fl tc ...</td>\n",
              "      <td>100595</td>\n",
              "      <td>.\\nNote: This question is quite challenging, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1449</th>\n",
              "      <td>Advection fog is most likely to form when:</td>\n",
              "      <td>[{'id': 1, 'answer': 'maritime warm air flows ...</td>\n",
              "      <td>a mild moist airstream flows over snow covered...</td>\n",
              "      <td>Refer to figure.\\nADVECTION FOG\\n. Is formed b...</td>\n",
              "      <td>&lt;div class=\"mt10 fl wd100\"&gt; &lt;div class=\"fl tc ...</td>\n",
              "      <td>73249</td>\n",
              "      <td>.\\nADVECTION FOG\\n. Is formed by the advection...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1450</th>\n",
              "      <td>Immediately following an extended flight at hi...</td>\n",
              "      <td>[{'id': 1, 'answer': 'Mixed ice on the leading...</td>\n",
              "      <td>Frost forming around the fuel tanks due to col...</td>\n",
              "      <td>Refer to figure.\\nIn this scenario, the aircra...</td>\n",
              "      <td>&lt;div class=\"mt10 fl wd100\"&gt; &lt;div class=\"fl tc ...</td>\n",
              "      <td>82240</td>\n",
              "      <td>.\\nIn this scenario, the aircraft has a very c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1451</th>\n",
              "      <td>The temperature at FL80 is +6°C. What will the...</td>\n",
              "      <td>[{'id': 1, 'answer': '0°C'}, {'id': 2, 'answer...</td>\n",
              "      <td>-4°C</td>\n",
              "      <td>In the standard troposphere air temperature on...</td>\n",
              "      <td></td>\n",
              "      <td>73082</td>\n",
              "      <td>In the standard troposphere air temperature on...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1452 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62dccbe8-13b8-4924-bc95-dd79a3c49eb3')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-62dccbe8-13b8-4924-bc95-dd79a3c49eb3 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-62dccbe8-13b8-4924-bc95-dd79a3c49eb3');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f030ccf9-0dbb-4d36-ac54-ab0534c8bd07\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f030ccf9-0dbb-4d36-ac54-ab0534c8bd07')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f030ccf9-0dbb-4d36-ac54-ab0534c8bd07 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                               question  \\\n",
              "0                   Mountain waves should be expected..   \n",
              "1     Which cloud poses the highest risk to aviation...   \n",
              "2     According to ICAO Annex 3 definition, \"visibil...   \n",
              "3     An aerodrome on the coast (Faro, Portugal) rep...   \n",
              "4     Which of the following statements concerning a...   \n",
              "...                                                 ...   \n",
              "1447  A mountain range spans from North to South. A ...   \n",
              "1448  Which type of cloud is most likely to be encou...   \n",
              "1449         Advection fog is most likely to form when:   \n",
              "1450  Immediately following an extended flight at hi...   \n",
              "1451  The temperature at FL80 is +6°C. What will the...   \n",
              "\n",
              "                                                answers  \\\n",
              "0     [{'id': 1, 'answer': 'directly over the mounta...   \n",
              "1     [{'id': 1, 'answer': 'Cumulus fractus.'}, {'id...   \n",
              "2     [{'id': 1, 'answer': '1 is correct, 2 is corre...   \n",
              "3     [{'id': 1, 'answer': 'Humidity reducing becaus...   \n",
              "4     [{'id': 1, 'answer': 'Air parcels that are for...   \n",
              "...                                                 ...   \n",
              "1447  [{'id': 1, 'answer': 'Cloud base higher than 2...   \n",
              "1448  [{'id': 1, 'answer': 'Altocumulus'}, {'id': 2,...   \n",
              "1449  [{'id': 1, 'answer': 'maritime warm air flows ...   \n",
              "1450  [{'id': 1, 'answer': 'Mixed ice on the leading...   \n",
              "1451  [{'id': 1, 'answer': '0°C'}, {'id': 2, 'answer...   \n",
              "\n",
              "                                         correct_answer  \\\n",
              "0           on the downwind side of the mountain range.   \n",
              "1                                         Cumulonimbus.   \n",
              "2                         1 is correct, 2 is incorrect.   \n",
              "3                       A cold front crossing the area.   \n",
              "4             An isothermal layer is absolutely stable.   \n",
              "...                                                 ...   \n",
              "1447  Cloud base higher than 2 000 ft and colder air...   \n",
              "1448                                        Altocumulus   \n",
              "1449  a mild moist airstream flows over snow covered...   \n",
              "1450  Frost forming around the fuel tanks due to col...   \n",
              "1451                                               -4°C   \n",
              "\n",
              "                                            explanation  \\\n",
              "0     Refer to figure.\\nMOUNTAIN/LEE WAVES\\ndevelop ...   \n",
              "1     Refer to figure.\\nWhen cold air displaces warm...   \n",
              "2     In effect,\\nvisibility\\nis a measure of atmosp...   \n",
              "3     Refer to figure.\\nNote: This question has two ...   \n",
              "4     An isothermal layer is defined as a vertical c...   \n",
              "...                                                 ...   \n",
              "1447  Refer to figure.\\nThis question is a little tr...   \n",
              "1448  Refer to figure.\\nNote: This question is quite...   \n",
              "1449  Refer to figure.\\nADVECTION FOG\\n. Is formed b...   \n",
              "1450  Refer to figure.\\nIn this scenario, the aircra...   \n",
              "1451  In the standard troposphere air temperature on...   \n",
              "\n",
              "                                   explanationImageHtml      id  \\\n",
              "0     <div class=\"mt10 fl wd100\"> <div class=\"fl tc ...   72880   \n",
              "1     <div class=\"mt10 fl wd100\"> <div class=\"fl tc ...   96266   \n",
              "2                                                         96218   \n",
              "3     <div class=\"mt10 fl wd100\"> <div class=\"fl tc ...   80606   \n",
              "4                                                         72780   \n",
              "...                                                 ...     ...   \n",
              "1447  <div class=\"mt10 fl wd100\"> <div class=\"fl tc ...   82192   \n",
              "1448  <div class=\"mt10 fl wd100\"> <div class=\"fl tc ...  100595   \n",
              "1449  <div class=\"mt10 fl wd100\"> <div class=\"fl tc ...   73249   \n",
              "1450  <div class=\"mt10 fl wd100\"> <div class=\"fl tc ...   82240   \n",
              "1451                                                      73082   \n",
              "\n",
              "                                      clean_explanation  \n",
              "0     .\\nMOUNTAIN/LEE WAVES\\ndevelop on the lee, or ...  \n",
              "1     .\\nWhen cold air displaces warm air, it create...  \n",
              "2     In effect,\\nvisibility\\nis a measure of atmosp...  \n",
              "3     .\\nNote: This question has two correct answers...  \n",
              "4     An isothermal layer is defined as a vertical c...  \n",
              "...                                                 ...  \n",
              "1447  .\\nThis question is a little tricky as it is a...  \n",
              "1448  .\\nNote: This question is quite challenging, a...  \n",
              "1449  .\\nADVECTION FOG\\n. Is formed by the advection...  \n",
              "1450  .\\nIn this scenario, the aircraft has a very c...  \n",
              "1451  In the standard troposphere air temperature on...  \n",
              "\n",
              "[1452 rows x 7 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "clean_df = pd.read_json('/content/cleanquest.json')\n",
        "clean_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "TnRDh0WTVBxf",
      "metadata": {
        "id": "TnRDh0WTVBxf"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train=clean_df.head(1000)\n",
        "test=clean_df.tail(452)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train)\n",
        "val_dataset=Dataset.from_pandas(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "QLraksfB8xLR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLraksfB8xLR",
        "outputId": "8b950968-162e-4914-9414-044b4ad5ae77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m803.3/803.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m205.3/205.3 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q langchain\n",
        "!pip install -q torch\n",
        "!pip install -q transformers\n",
        "!pip install -q sentence-transformers\n",
        "!pip install -q datasets\n",
        "!pip install -q faiss-cpu\n",
        "!pip install -q pypdf\n",
        "!pip install -q rouge_score\n",
        "!pip install -q evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "cnsPO4VS88cc",
      "metadata": {
        "id": "cnsPO4VS88cc"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import TextLoader,PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter,TextSplitter,CharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "from transformers import AutoTokenizer, pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.schema import Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "RZOxcUDEVBxg",
      "metadata": {
        "id": "RZOxcUDEVBxg"
      },
      "outputs": [],
      "source": [
        "# LOAD: explanations as the contextt db\n",
        "list_of_documents=[]\n",
        "for clean_exp in train.clean_explanation.values:\n",
        "  text_splitter = RecursiveCharacterTextSplitter(chunk_size=256,chunk_overlap=16)\n",
        "\n",
        "  list_of_documents.extend(text_splitter.split_documents([Document(page_content=clean_exp)]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "5_AB_GB-VBxg",
      "metadata": {
        "id": "5_AB_GB-VBxg"
      },
      "outputs": [],
      "source": [
        "for pdf in [\"/path_to_book1.pdf\",\"/path_to_book2.pdf\"]:\n",
        "  loader = PyPDFLoader(pdf)\n",
        "  pages = loader.load_and_split()\n",
        "  list_of_documents.extend(text_splitter.split_documents(pages))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "oHn5_KhTVBxh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHn5_KhTVBxh",
        "outputId": "429d30e1-7603-455f-f268-1772ca5f8378"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        }
      ],
      "source": [
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "#modelPath = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "\n",
        "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
        "model_kwargs = {'device':'cpu'}\n",
        "\n",
        "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "\n",
        "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=modelPath,     # Provide the pre-trained model's path\n",
        "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
        "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "FI9kJu4WVBxh",
      "metadata": {
        "id": "FI9kJu4WVBxh"
      },
      "outputs": [],
      "source": [
        "db = FAISS.from_documents(list_of_documents, embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ev4fzKMVYfar",
      "metadata": {
        "id": "Ev4fzKMVYfar"
      },
      "source": [
        "Filtering step as it was observed some pages consisted of questions which was adding noisy documents as relevant in the retrieval phase of the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1kawE8cUrIQa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kawE8cUrIQa",
        "outputId": "0414b6fd-5c1a-4903-d0f7-9158b275f53c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pages_toremove=[15,16,17,18,19, 28, 29, 30, 31, 32, 33,40,41, 54, 55, 56, 57, 58,59,76,77,78,79,88,89, 90,91,102,103,104,105, 106,107,124,125,126,127,128,129,143,144,145,146,147,148,149,150,151,152,153,174,175,176,177,178,179,180,181,191,192,193,194,195,196,211,212,213,214,215,216,217,218,219,230,231,232,234,235,251,252,253,254,270,271,283,284,285,286,287,288,289,301,302,303,304,305,323,324,325,326,327,328,329,330,331,339,340,341,342,343,344,345,346,347,348,366,367,368,369,370,371,397,398,399,400,401,402,403,404,405,406,407,408,409,418,419,486,487,488,489,490,496,497,498,499,500,512,513,514,515,516,517,518,519,520,528,529,530,531,532,533,534,535,536,537,538,539,540,541]+[num for num in range(550,664)]\n",
        "keys_to_remove=[]\n",
        "for key in db.docstore._dict.keys():\n",
        "  if 'page' in db.docstore._dict[key].metadata.keys() and db.docstore._dict[key].metadata['source']=='/content/vdoc.pub_cae-oxford-aviation-academy-atpl-book-9-meteorology.pdf':\n",
        "    if db.docstore._dict[key].metadata['page'] in pages_toremove:\n",
        "      keys_to_remove.append(key)\n",
        "db.delete(keys_to_remove)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "FIZ5nvpdVBxf",
      "metadata": {
        "id": "FIZ5nvpdVBxf"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(question, answer, max_length=128):\n",
        "  inputs = tokenizer(\n",
        "  question,\n",
        "  max_length=max_length,\n",
        "  padding='max_length',\n",
        "  truncation=True,\n",
        "  return_tensors='pt'\n",
        "  )\n",
        "  input_ids = inputs['input_ids'].squeeze()\n",
        "  attention_mask = inputs['attention_mask'].squeeze()\n",
        "\n",
        "  # Encode the answer\n",
        "\n",
        "  # Tokenize targets with text_target=...\n",
        "  labels = tokenizer(text_target=answer, max_length=max_length, padding='max_length', truncation=True)\n",
        "\n",
        "  # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
        "  # padding in the loss.\n",
        "\n",
        "  labels[\"input_ids\"] = [(l if l != tokenizer.pad_token_id else -100) for l in labels[\"input_ids\"]]\n",
        "\n",
        "  inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "  return {\n",
        "      'input_ids': input_ids,\n",
        "      'attention_mask': attention_mask,\n",
        "      'labels': inputs[\"labels\"]\n",
        "\n",
        "  }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "RuiOGNxlVBxh",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuiOGNxlVBxh",
        "outputId": "43e06f7b-8b45-4920-bff5-4bf26845aa3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:04<00:00, 15.52it/s]\n",
            "100%|██████████| 452/452 [00:28<00:00, 15.83it/s]\n"
          ]
        }
      ],
      "source": [
        "preprocessed_train_dataset = []\n",
        "preprocessed_val_dataset = []\n",
        "from tqdm import tqdm\n",
        "for example in tqdm(train_dataset):\n",
        "  question=example['question']\n",
        "  option1=example['answers'][0]['answer']\n",
        "  option2=example['answers'][1]['answer']\n",
        "  option3=example['answers'][2]['answer']\n",
        "  option4=example['answers'][3]['answer']\n",
        "  context=''\n",
        "  question=\"For the question: \"+ question +\" ,choose the correct answer from the following answers: option1) \" + option1 +\", option2) \"+ option2 +\", option3) \"+ option3 + \", option4) \"+option4\n",
        "  for docs in db.search(question,search_type='mmr',k=10):\n",
        "    context+=docs.page_content+'\\n'\n",
        "\n",
        "  question=\"context: \"+context+ \" Use the context to answer the following question. Answer using the context only. \"+ question\n",
        "  preprocessed_example = preprocess_data(question, example['correct_answer'])\n",
        "  preprocessed_train_dataset.append(preprocessed_example)\n",
        "\n",
        "for example in tqdm(val_dataset):\n",
        "  question=example['question']\n",
        "  option1=example['answers'][0]['answer']\n",
        "  option2=example['answers'][1]['answer']\n",
        "  option3=example['answers'][2]['answer']\n",
        "  option4=example['answers'][3]['answer']\n",
        "  context=''\n",
        "  question=\"For the question: \"+ question +\" ,choose the correct answer from the following answers: option1) \" + option1 +\", option2) \"+ option2 +\", option3) \"+ option3 + \", option4) \"+option4\n",
        "\n",
        "  for docs in db.search(question,search_type='mmr',k=10):\n",
        "    context+=docs.page_content+'\\n'\n",
        "  question=\"context: \"+context+ \" Use the context to answer the following question. Answer using the context only. \"+question\n",
        "  preprocessed_example = preprocess_data(question, example['correct_answer'])\n",
        "  preprocessed_val_dataset.append(preprocessed_example)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "aFvpasFhVBxi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFvpasFhVBxi",
        "outputId": "ad94b9ee-fea1-4763-c9ff-241ff490b765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 1000\n",
            "})\n",
            "Dataset({\n",
            "    features: ['input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 452\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "tokenized_train_dataset = Dataset.from_dict(\n",
        "{key: [example[key] for example in preprocessed_train_dataset] for key in preprocessed_train_dataset[0].keys()})\n",
        "print(tokenized_train_dataset)\n",
        "\n",
        "tokenized_val_dataset = Dataset.from_dict(\n",
        "{key: [example[key] for example in preprocessed_val_dataset] for key in preprocessed_val_dataset[0].keys()})\n",
        "print(tokenized_val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qzwyi-Z9yzRF",
      "metadata": {
        "id": "qzwyi-Z9yzRF"
      },
      "source": [
        "Let's also apply some pre-processing of the input data, the labels needs to be pre-processed, the tokens corresponding to `pad_token_id` needs to be set to `-100` so that the `CrossEntropy` loss associated with the model will correctly ignore these tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcNTdVypGEPb",
      "metadata": {
        "id": "bcNTdVypGEPb"
      },
      "source": [
        "## Finetuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "eCpYt1V7Ervx",
      "metadata": {
        "id": "eCpYt1V7Ervx"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq,T5ForConditionalGeneration, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "aKVljuAzO_1H",
      "metadata": {
        "id": "aKVljuAzO_1H"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "bhQt1MFAW5DW",
      "metadata": {
        "id": "bhQt1MFAW5DW"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import evaluate\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "metric = evaluate.load(\"rouge\")\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "ohXEYM-0EgMz",
      "metadata": {
        "id": "ohXEYM-0EgMz"
      },
      "outputs": [],
      "source": [
        "# Global Parameters\n",
        "L_RATE = 15e-6 #3e-4 #15e-6,\n",
        "BATCH_SIZE = 4\n",
        "PER_DEVICE_EVAL_BATCH = 4\n",
        "WEIGHT_DECAY = 0.02\n",
        "SAVE_TOTAL_LIM = 3\n",
        "NUM_EPOCHS = 8\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir=\"./resultss\",\n",
        "   evaluation_strategy=\"epoch\",\n",
        "   learning_rate=L_RATE,\n",
        "   per_device_train_batch_size=BATCH_SIZE,\n",
        "   per_device_eval_batch_size=PER_DEVICE_EVAL_BATCH,\n",
        "   weight_decay=WEIGHT_DECAY,\n",
        "   save_total_limit=SAVE_TOTAL_LIM,\n",
        "   num_train_epochs=NUM_EPOCHS,\n",
        "   predict_with_generate=True,\n",
        "   push_to_hub=False,\n",
        "   save_strategy='epoch',\n",
        "   load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train_dataset,\n",
        "   eval_dataset=tokenized_val_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "ab52b651",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ab52b651",
        "outputId": "ca7c25a8-3b47-4021-a99a-3c3d8590a958"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2000/2000 22:52, Epoch 8/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.361646</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>3.353400</td>\n",
              "      <td>3.244041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>3.353400</td>\n",
              "      <td>3.155964</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.157500</td>\n",
              "      <td>3.101147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.157500</td>\n",
              "      <td>3.070980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.083800</td>\n",
              "      <td>3.053898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.083800</td>\n",
              "      <td>3.044508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.060700</td>\n",
              "      <td>3.041618</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2000, training_loss=3.1638834228515624, metrics={'train_runtime': 1374.4203, 'train_samples_per_second': 5.821, 'train_steps_per_second': 1.455, 'total_flos': 1380386340864000.0, 'train_loss': 3.1638834228515624, 'epoch': 8.0})"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "rL2y2nHHJmTf",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL2y2nHHJmTf",
        "outputId": "a74a20b8-fe75-42b2-e5b6-0c6520176012"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "last_checkpoint = '/content/resultss/checkpoint-2000'\n",
        "\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(last_checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(last_checkpoint)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "UD-R8E2dJDYN",
      "metadata": {
        "id": "UD-R8E2dJDYN"
      },
      "outputs": [],
      "source": [
        "question_answerer = pipeline(\n",
        "    \"text2text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
        "# with additional model-specific arguments (temperature and max_length)\n",
        "llm = HuggingFacePipeline(\n",
        "    pipeline=question_answerer,\n",
        "    model_kwargs={\"temperature\": 0.1, \"max_length\": 512},\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "K_ZOkMyuJOR-",
      "metadata": {
        "id": "K_ZOkMyuJOR-"
      },
      "outputs": [],
      "source": [
        "retriever = db.as_retriever(\n",
        "     search_kwargs={\"k\":10 }, search_type = 'mmr'\n",
        ")\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, retriever=retriever, return_source_documents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "urYA9ncVJRXw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urYA9ncVJRXw",
        "outputId": "5396114a-e69e-4c42-9e05-1c70ba1fcf72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/452 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  0%|          | 1/452 [00:05<39:25,  5.25s/it]Token indices sequence length is longer than the specified maximum sequence length for this model (547 > 512). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  0%|          | 2/452 [00:09<34:36,  4.62s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  1%|          | 3/452 [00:13<31:38,  4.23s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  1%|          | 4/452 [00:15<27:05,  3.63s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  1%|          | 5/452 [00:19<25:49,  3.47s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  1%|▏         | 6/452 [00:21<24:19,  3.27s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  2%|▏         | 7/452 [00:24<23:33,  3.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  2%|▏         | 8/452 [00:28<23:42,  3.20s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  2%|▏         | 9/452 [00:31<24:03,  3.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  2%|▏         | 10/452 [00:34<22:26,  3.05s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  2%|▏         | 11/452 [00:37<23:06,  3.14s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  3%|▎         | 12/452 [00:41<25:59,  3.54s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  3%|▎         | 13/452 [00:46<27:20,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  3%|▎         | 14/452 [00:48<24:28,  3.35s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  3%|▎         | 15/452 [00:52<24:44,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  4%|▎         | 16/452 [00:55<24:43,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  4%|▍         | 17/452 [00:59<26:02,  3.59s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  4%|▍         | 18/452 [01:04<28:43,  3.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  4%|▍         | 19/452 [01:11<36:07,  5.01s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  4%|▍         | 20/452 [01:18<39:11,  5.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  5%|▍         | 21/452 [01:28<48:17,  6.72s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  5%|▍         | 22/452 [01:31<41:00,  5.72s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  5%|▌         | 23/452 [01:34<35:33,  4.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  5%|▌         | 24/452 [01:39<34:45,  4.87s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  6%|▌         | 25/452 [01:42<31:17,  4.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  6%|▌         | 26/452 [01:46<29:57,  4.22s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  6%|▌         | 27/452 [01:49<28:04,  3.96s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  6%|▌         | 28/452 [01:53<26:53,  3.81s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  6%|▋         | 29/452 [01:58<29:41,  4.21s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  7%|▋         | 30/452 [02:01<28:19,  4.03s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  7%|▋         | 31/452 [02:06<29:49,  4.25s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  7%|▋         | 32/452 [02:09<25:40,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  7%|▋         | 33/452 [02:11<23:25,  3.35s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  8%|▊         | 34/452 [02:14<21:53,  3.14s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  8%|▊         | 35/452 [02:18<24:28,  3.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  8%|▊         | 36/452 [02:23<27:53,  4.02s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  8%|▊         | 37/452 [02:28<28:26,  4.11s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  8%|▊         | 38/452 [02:33<30:10,  4.37s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  9%|▊         | 39/452 [02:36<27:47,  4.04s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  9%|▉         | 40/452 [02:41<30:32,  4.45s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  9%|▉         | 41/452 [02:45<28:25,  4.15s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "  9%|▉         | 42/452 [02:48<26:03,  3.81s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 10%|▉         | 43/452 [02:51<25:37,  3.76s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 10%|▉         | 44/452 [02:55<24:19,  3.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 10%|▉         | 45/452 [02:59<25:16,  3.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 10%|█         | 46/452 [03:02<25:03,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 10%|█         | 47/452 [03:07<26:36,  3.94s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 11%|█         | 48/452 [03:10<25:04,  3.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 11%|█         | 49/452 [03:13<22:59,  3.42s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 11%|█         | 50/452 [03:17<24:33,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 11%|█▏        | 51/452 [03:20<22:52,  3.42s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 12%|█▏        | 52/452 [03:24<23:30,  3.53s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 12%|█▏        | 53/452 [03:27<22:10,  3.33s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 12%|█▏        | 54/452 [03:30<22:49,  3.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 12%|█▏        | 55/452 [03:33<22:01,  3.33s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 12%|█▏        | 56/452 [03:38<24:31,  3.72s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 13%|█▎        | 57/452 [03:42<24:52,  3.78s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 13%|█▎        | 58/452 [03:44<22:21,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 13%|█▎        | 59/452 [03:50<26:01,  3.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 13%|█▎        | 60/452 [03:53<24:56,  3.82s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 13%|█▎        | 61/452 [03:56<22:14,  3.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 14%|█▎        | 62/452 [03:59<21:37,  3.33s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 14%|█▍        | 63/452 [04:04<25:25,  3.92s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 14%|█▍        | 64/452 [04:06<22:25,  3.47s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 14%|█▍        | 65/452 [04:11<24:57,  3.87s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 15%|█▍        | 66/452 [04:15<24:25,  3.80s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 15%|█▍        | 67/452 [04:18<22:19,  3.48s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 15%|█▌        | 68/452 [04:21<22:40,  3.54s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 15%|█▌        | 69/452 [04:24<20:57,  3.28s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 15%|█▌        | 70/452 [04:29<23:21,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 16%|█▌        | 71/452 [04:33<24:03,  3.79s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 16%|█▌        | 72/452 [04:36<22:55,  3.62s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 16%|█▌        | 73/452 [04:40<23:51,  3.78s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 16%|█▋        | 74/452 [04:43<22:44,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 17%|█▋        | 75/452 [04:47<23:05,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 17%|█▋        | 76/452 [04:50<22:19,  3.56s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 17%|█▋        | 77/452 [04:55<23:59,  3.84s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 17%|█▋        | 78/452 [04:58<21:55,  3.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 17%|█▋        | 79/452 [05:00<20:21,  3.27s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 18%|█▊        | 80/452 [05:03<19:29,  3.14s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 18%|█▊        | 81/452 [05:07<21:25,  3.47s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 18%|█▊        | 82/452 [05:11<21:03,  3.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 18%|█▊        | 83/452 [05:14<20:58,  3.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 19%|█▊        | 84/452 [05:17<20:47,  3.39s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 19%|█▉        | 85/452 [05:21<21:58,  3.59s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 19%|█▉        | 86/452 [05:25<21:23,  3.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 19%|█▉        | 87/452 [05:28<20:40,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 19%|█▉        | 88/452 [05:33<23:49,  3.93s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 20%|█▉        | 89/452 [05:37<24:30,  4.05s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 20%|█▉        | 90/452 [05:41<23:55,  3.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 20%|██        | 91/452 [05:44<21:05,  3.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 20%|██        | 92/452 [05:49<24:05,  4.02s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 21%|██        | 93/452 [05:53<24:00,  4.01s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 21%|██        | 94/452 [05:56<22:42,  3.81s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 21%|██        | 95/452 [06:00<22:24,  3.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 21%|██        | 96/452 [06:03<21:08,  3.56s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 21%|██▏       | 97/452 [06:07<21:20,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 22%|██▏       | 98/452 [06:09<19:25,  3.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 22%|██▏       | 99/452 [06:14<22:51,  3.88s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 22%|██▏       | 100/452 [06:19<23:33,  4.02s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 22%|██▏       | 101/452 [06:23<23:53,  4.08s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 23%|██▎       | 102/452 [06:28<25:14,  4.33s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 23%|██▎       | 103/452 [06:31<23:01,  3.96s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 23%|██▎       | 104/452 [06:34<20:50,  3.59s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 23%|██▎       | 105/452 [06:36<18:59,  3.28s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 23%|██▎       | 106/452 [06:41<21:49,  3.78s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 24%|██▎       | 107/452 [06:45<21:40,  3.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 24%|██▍       | 108/452 [06:48<20:49,  3.63s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 24%|██▍       | 109/452 [06:53<22:26,  3.93s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 24%|██▍       | 110/452 [06:55<19:57,  3.50s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 25%|██▍       | 111/452 [06:59<20:30,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 25%|██▍       | 112/452 [07:02<19:05,  3.37s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 25%|██▌       | 113/452 [07:06<20:02,  3.55s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 25%|██▌       | 114/452 [07:10<21:20,  3.79s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 25%|██▌       | 115/452 [07:13<19:47,  3.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 26%|██▌       | 116/452 [07:15<17:29,  3.12s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 26%|██▌       | 117/452 [07:20<19:23,  3.47s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 26%|██▌       | 118/452 [07:23<18:56,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 26%|██▋       | 119/452 [07:26<17:52,  3.22s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 27%|██▋       | 120/452 [07:30<19:02,  3.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 27%|██▋       | 121/452 [07:34<19:54,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 27%|██▋       | 122/452 [07:39<22:20,  4.06s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 27%|██▋       | 123/452 [07:42<20:30,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 27%|██▋       | 124/452 [07:48<24:31,  4.48s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 28%|██▊       | 125/452 [07:51<22:31,  4.13s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 28%|██▊       | 126/452 [07:54<20:27,  3.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 28%|██▊       | 127/452 [07:59<22:39,  4.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 28%|██▊       | 128/452 [08:03<22:07,  4.10s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 29%|██▊       | 129/452 [08:08<22:22,  4.15s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 29%|██▉       | 130/452 [08:12<21:57,  4.09s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 29%|██▉       | 131/452 [08:16<21:39,  4.05s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 29%|██▉       | 132/452 [08:19<20:43,  3.88s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 29%|██▉       | 133/452 [08:22<19:17,  3.63s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 30%|██▉       | 134/452 [08:27<21:23,  4.04s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 30%|██▉       | 135/452 [08:30<20:10,  3.82s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 30%|███       | 136/452 [08:33<18:13,  3.46s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 30%|███       | 137/452 [08:38<20:53,  3.98s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 31%|███       | 138/452 [08:42<20:57,  4.00s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 31%|███       | 139/452 [08:46<20:07,  3.86s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 31%|███       | 140/452 [08:50<20:22,  3.92s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 31%|███       | 141/452 [08:53<19:42,  3.80s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 31%|███▏      | 142/452 [08:57<19:16,  3.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 32%|███▏      | 143/452 [09:01<20:21,  3.95s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 32%|███▏      | 144/452 [09:05<20:28,  3.99s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 32%|███▏      | 145/452 [09:12<23:44,  4.64s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 32%|███▏      | 146/452 [09:14<20:46,  4.07s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 33%|███▎      | 147/452 [09:19<21:24,  4.21s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 33%|███▎      | 148/452 [09:24<23:13,  4.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 33%|███▎      | 149/452 [09:27<19:47,  3.92s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 33%|███▎      | 150/452 [09:31<20:24,  4.05s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 33%|███▎      | 151/452 [09:35<20:20,  4.06s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 34%|███▎      | 152/452 [09:38<19:04,  3.82s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 34%|███▍      | 153/452 [09:41<17:49,  3.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 34%|███▍      | 154/452 [09:46<18:44,  3.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 34%|███▍      | 155/452 [09:48<17:01,  3.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 35%|███▍      | 156/452 [09:52<17:48,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 35%|███▍      | 157/452 [09:57<19:20,  3.93s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 35%|███▍      | 158/452 [10:00<18:13,  3.72s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 35%|███▌      | 159/452 [10:03<16:59,  3.48s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 35%|███▌      | 160/452 [10:06<15:46,  3.24s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 36%|███▌      | 161/452 [10:10<17:26,  3.60s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 36%|███▌      | 162/452 [10:13<16:01,  3.32s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 36%|███▌      | 163/452 [10:17<16:38,  3.45s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 36%|███▋      | 164/452 [10:20<16:02,  3.34s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 37%|███▋      | 165/452 [10:24<17:18,  3.62s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 37%|███▋      | 166/452 [10:28<17:51,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 37%|███▋      | 167/452 [10:31<17:10,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 37%|███▋      | 168/452 [10:36<18:48,  3.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 37%|███▋      | 169/452 [10:40<18:24,  3.90s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 38%|███▊      | 170/452 [10:45<19:58,  4.25s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 38%|███▊      | 171/452 [10:49<19:56,  4.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 38%|███▊      | 172/452 [10:52<18:10,  3.89s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 38%|███▊      | 173/452 [10:56<17:52,  3.84s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 38%|███▊      | 174/452 [11:02<21:15,  4.59s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 39%|███▊      | 175/452 [11:05<18:06,  3.92s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 39%|███▉      | 176/452 [11:09<18:16,  3.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 39%|███▉      | 177/452 [11:12<16:30,  3.60s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 39%|███▉      | 178/452 [11:16<18:08,  3.97s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 40%|███▉      | 179/452 [11:21<18:24,  4.05s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 40%|███▉      | 180/452 [11:23<16:26,  3.63s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 40%|████      | 181/452 [11:28<17:29,  3.87s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 40%|████      | 182/452 [11:31<17:05,  3.80s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 40%|████      | 183/452 [11:34<15:06,  3.37s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 41%|████      | 184/452 [11:38<15:44,  3.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 41%|████      | 185/452 [11:41<15:59,  3.60s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 41%|████      | 186/452 [11:44<15:04,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 41%|████▏     | 187/452 [11:48<15:23,  3.49s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 42%|████▏     | 188/452 [11:51<14:48,  3.36s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 42%|████▏     | 189/452 [11:55<15:33,  3.55s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 42%|████▏     | 190/452 [11:59<15:31,  3.56s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 42%|████▏     | 191/452 [12:02<14:46,  3.39s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 42%|████▏     | 192/452 [12:05<14:35,  3.37s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 43%|████▎     | 193/452 [12:08<14:04,  3.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 43%|████▎     | 194/452 [12:12<15:02,  3.50s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 43%|████▎     | 195/452 [12:15<13:57,  3.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 43%|████▎     | 196/452 [12:17<12:55,  3.03s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 44%|████▎     | 197/452 [12:21<14:16,  3.36s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 44%|████▍     | 198/452 [12:24<13:07,  3.10s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 44%|████▍     | 199/452 [12:27<12:49,  3.04s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 44%|████▍     | 200/452 [12:30<12:36,  3.00s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 44%|████▍     | 201/452 [12:33<13:03,  3.12s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 45%|████▍     | 202/452 [12:38<14:39,  3.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 45%|████▍     | 203/452 [12:40<13:36,  3.28s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 45%|████▌     | 204/452 [12:44<13:35,  3.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 45%|████▌     | 205/452 [12:49<15:45,  3.83s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 46%|████▌     | 206/452 [12:52<15:10,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 46%|████▌     | 207/452 [12:56<15:33,  3.81s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 46%|████▌     | 208/452 [13:00<16:00,  3.94s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 46%|████▌     | 209/452 [13:03<14:43,  3.63s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 46%|████▋     | 210/452 [13:07<14:16,  3.54s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 47%|████▋     | 211/452 [13:10<14:26,  3.60s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 47%|████▋     | 212/452 [13:15<15:45,  3.94s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 47%|████▋     | 213/452 [13:19<15:55,  4.00s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 47%|████▋     | 214/452 [13:22<14:41,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 48%|████▊     | 215/452 [13:26<14:38,  3.71s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 48%|████▊     | 216/452 [13:29<13:42,  3.49s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 48%|████▊     | 217/452 [13:32<12:50,  3.28s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 48%|████▊     | 218/452 [13:35<12:38,  3.24s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 48%|████▊     | 219/452 [13:39<13:27,  3.47s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 49%|████▊     | 220/452 [13:41<12:27,  3.22s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 49%|████▉     | 221/452 [13:44<11:52,  3.09s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 49%|████▉     | 222/452 [13:48<12:45,  3.33s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 49%|████▉     | 223/452 [13:54<15:57,  4.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 50%|████▉     | 224/452 [13:58<15:29,  4.08s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 50%|████▉     | 225/452 [14:01<14:01,  3.71s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 50%|█████     | 226/452 [14:06<15:10,  4.03s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 50%|█████     | 227/452 [14:09<13:44,  3.66s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 50%|█████     | 228/452 [14:11<12:28,  3.34s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 51%|█████     | 229/452 [14:14<11:34,  3.11s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 51%|█████     | 230/452 [14:19<13:46,  3.72s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 51%|█████     | 231/452 [14:22<12:42,  3.45s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 51%|█████▏    | 232/452 [14:26<13:54,  3.79s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 52%|█████▏    | 233/452 [14:30<13:17,  3.64s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 52%|█████▏    | 234/452 [14:33<13:20,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 52%|█████▏    | 235/452 [14:36<11:55,  3.30s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 52%|█████▏    | 236/452 [14:39<11:37,  3.23s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 52%|█████▏    | 237/452 [14:43<12:28,  3.48s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 53%|█████▎    | 238/452 [14:46<11:50,  3.32s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 53%|█████▎    | 239/452 [14:50<12:25,  3.50s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 53%|█████▎    | 240/452 [14:54<13:13,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 53%|█████▎    | 241/452 [14:59<14:25,  4.10s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 54%|█████▎    | 242/452 [15:02<12:50,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 54%|█████▍    | 243/452 [15:05<12:03,  3.46s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 54%|█████▍    | 244/452 [15:08<11:23,  3.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 54%|█████▍    | 245/452 [15:12<12:57,  3.76s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 54%|█████▍    | 246/452 [15:16<12:55,  3.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 55%|█████▍    | 247/452 [15:19<11:56,  3.50s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 55%|█████▍    | 248/452 [15:23<12:10,  3.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 55%|█████▌    | 249/452 [15:26<11:51,  3.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 55%|█████▌    | 250/452 [15:29<11:20,  3.37s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 56%|█████▌    | 251/452 [15:32<10:17,  3.07s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 56%|█████▌    | 252/452 [15:35<10:10,  3.05s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 56%|█████▌    | 253/452 [15:38<10:39,  3.21s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 56%|█████▌    | 254/452 [15:42<11:03,  3.35s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 56%|█████▋    | 255/452 [15:45<10:23,  3.16s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 57%|█████▋    | 256/452 [15:48<10:38,  3.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 57%|█████▋    | 257/452 [15:51<10:07,  3.11s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 57%|█████▋    | 258/452 [15:54<10:06,  3.13s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 57%|█████▋    | 259/452 [15:58<11:01,  3.43s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 58%|█████▊    | 260/452 [16:01<10:49,  3.38s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 58%|█████▊    | 261/452 [16:04<09:54,  3.11s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 58%|█████▊    | 262/452 [16:07<09:25,  2.98s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 58%|█████▊    | 263/452 [16:10<10:02,  3.19s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 58%|█████▊    | 264/452 [16:14<10:13,  3.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 59%|█████▊    | 265/452 [16:17<10:15,  3.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 59%|█████▉    | 266/452 [16:21<10:30,  3.39s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 59%|█████▉    | 267/452 [16:24<10:31,  3.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 59%|█████▉    | 268/452 [16:27<09:59,  3.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 60%|█████▉    | 269/452 [16:31<10:48,  3.54s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 60%|█████▉    | 270/452 [16:35<10:38,  3.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 60%|█████▉    | 271/452 [16:38<10:21,  3.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 60%|██████    | 272/452 [16:42<10:32,  3.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 60%|██████    | 273/452 [16:45<10:07,  3.39s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 61%|██████    | 274/452 [16:48<09:58,  3.36s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 61%|██████    | 275/452 [16:52<10:11,  3.45s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 61%|██████    | 276/452 [16:56<11:13,  3.83s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 61%|██████▏   | 277/452 [17:00<10:47,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 62%|██████▏   | 278/452 [17:03<10:29,  3.62s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 62%|██████▏   | 279/452 [17:06<10:00,  3.47s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 62%|██████▏   | 280/452 [17:11<10:56,  3.82s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 62%|██████▏   | 281/452 [17:13<09:40,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 62%|██████▏   | 282/452 [17:16<09:08,  3.23s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 63%|██████▎   | 283/452 [17:21<10:23,  3.69s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 63%|██████▎   | 284/452 [17:24<09:36,  3.43s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 63%|██████▎   | 285/452 [17:26<08:50,  3.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 63%|██████▎   | 286/452 [17:30<09:25,  3.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 63%|██████▎   | 287/452 [17:36<11:03,  4.02s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 64%|██████▎   | 288/452 [17:39<10:18,  3.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 64%|██████▍   | 289/452 [17:43<10:24,  3.83s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 64%|██████▍   | 290/452 [17:48<11:13,  4.15s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 64%|██████▍   | 291/452 [17:53<11:50,  4.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 65%|██████▍   | 292/452 [17:55<09:58,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 65%|██████▍   | 293/452 [18:00<10:47,  4.07s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 65%|██████▌   | 294/452 [18:04<10:52,  4.13s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 65%|██████▌   | 295/452 [18:07<10:07,  3.87s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 65%|██████▌   | 296/452 [18:12<10:47,  4.15s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 66%|██████▌   | 297/452 [18:16<10:34,  4.09s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 66%|██████▌   | 298/452 [18:20<10:15,  4.00s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 66%|██████▌   | 299/452 [18:23<09:32,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 66%|██████▋   | 300/452 [18:28<10:29,  4.14s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 67%|██████▋   | 301/452 [18:31<09:22,  3.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 67%|██████▋   | 302/452 [18:33<08:17,  3.32s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 67%|██████▋   | 303/452 [18:36<07:53,  3.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 67%|██████▋   | 304/452 [18:40<08:25,  3.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 67%|██████▋   | 305/452 [18:43<07:58,  3.26s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 68%|██████▊   | 306/452 [18:45<07:23,  3.04s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 68%|██████▊   | 307/452 [18:49<07:47,  3.22s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 68%|██████▊   | 308/452 [18:55<09:19,  3.88s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 68%|██████▊   | 309/452 [18:57<08:10,  3.43s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 69%|██████▊   | 310/452 [19:00<07:36,  3.22s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 69%|██████▉   | 311/452 [19:03<07:27,  3.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 69%|██████▉   | 312/452 [19:08<08:50,  3.79s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 69%|██████▉   | 313/452 [19:11<08:08,  3.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 69%|██████▉   | 314/452 [19:14<07:53,  3.43s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 70%|██████▉   | 315/452 [19:19<08:57,  3.92s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 70%|██████▉   | 316/452 [19:22<08:13,  3.63s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 70%|███████   | 317/452 [19:26<08:19,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 70%|███████   | 318/452 [19:28<07:20,  3.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 71%|███████   | 319/452 [19:34<08:38,  3.90s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 71%|███████   | 320/452 [19:40<09:58,  4.54s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 71%|███████   | 321/452 [19:44<09:41,  4.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 71%|███████   | 322/452 [19:48<09:25,  4.35s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 71%|███████▏  | 323/452 [19:51<08:46,  4.08s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 72%|███████▏  | 324/452 [19:55<08:04,  3.79s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 72%|███████▏  | 325/452 [20:00<08:46,  4.15s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 72%|███████▏  | 326/452 [20:03<08:18,  3.96s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 72%|███████▏  | 327/452 [20:07<08:12,  3.94s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 73%|███████▎  | 328/452 [20:11<07:56,  3.84s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 73%|███████▎  | 329/452 [20:13<07:05,  3.46s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 73%|███████▎  | 330/452 [20:17<07:19,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 73%|███████▎  | 331/452 [20:21<07:17,  3.62s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 73%|███████▎  | 332/452 [20:24<07:16,  3.64s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 74%|███████▎  | 333/452 [20:28<07:13,  3.65s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 74%|███████▍  | 334/452 [20:31<06:38,  3.38s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 74%|███████▍  | 335/452 [20:34<06:33,  3.36s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 74%|███████▍  | 336/452 [20:39<07:25,  3.84s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 75%|███████▍  | 337/452 [20:44<07:47,  4.07s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 75%|███████▍  | 338/452 [20:46<06:57,  3.66s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 75%|███████▌  | 339/452 [20:52<07:56,  4.21s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 75%|███████▌  | 340/452 [20:54<06:50,  3.66s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 75%|███████▌  | 341/452 [20:58<06:48,  3.68s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 76%|███████▌  | 342/452 [21:02<06:46,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 76%|███████▌  | 343/452 [21:09<08:31,  4.69s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 76%|███████▌  | 344/452 [21:11<07:12,  4.00s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 76%|███████▋  | 345/452 [21:14<06:35,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 77%|███████▋  | 346/452 [21:19<07:01,  3.98s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 77%|███████▋  | 347/452 [21:21<06:17,  3.59s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 77%|███████▋  | 348/452 [21:25<05:59,  3.46s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 77%|███████▋  | 349/452 [21:29<06:35,  3.84s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 77%|███████▋  | 350/452 [21:32<06:08,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 78%|███████▊  | 351/452 [21:35<05:36,  3.33s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 78%|███████▊  | 352/452 [21:38<05:29,  3.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 78%|███████▊  | 353/452 [21:44<06:40,  4.05s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 78%|███████▊  | 354/452 [21:47<06:02,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 79%|███████▊  | 355/452 [21:49<05:13,  3.23s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 79%|███████▉  | 356/452 [21:53<05:37,  3.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 79%|███████▉  | 357/452 [21:57<05:33,  3.51s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 79%|███████▉  | 358/452 [22:00<05:10,  3.30s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 79%|███████▉  | 359/452 [22:03<05:15,  3.39s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 80%|███████▉  | 360/452 [22:08<05:57,  3.89s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 80%|███████▉  | 361/452 [22:11<05:28,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 80%|████████  | 362/452 [22:14<04:54,  3.27s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 80%|████████  | 363/452 [22:17<04:46,  3.21s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 81%|████████  | 364/452 [22:20<04:50,  3.30s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 81%|████████  | 365/452 [22:24<04:56,  3.41s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 81%|████████  | 366/452 [22:28<05:11,  3.62s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 81%|████████  | 367/452 [22:32<05:14,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 81%|████████▏ | 368/452 [22:36<05:23,  3.85s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 82%|████████▏ | 369/452 [22:42<06:00,  4.35s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 82%|████████▏ | 370/452 [22:46<06:03,  4.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 82%|████████▏ | 371/452 [22:50<05:52,  4.35s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 82%|████████▏ | 372/452 [22:54<05:18,  3.99s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 83%|████████▎ | 373/452 [22:58<05:18,  4.04s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 83%|████████▎ | 374/452 [23:01<05:07,  3.94s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 83%|████████▎ | 375/452 [23:06<05:13,  4.07s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 83%|████████▎ | 376/452 [23:09<04:40,  3.69s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 83%|████████▎ | 377/452 [23:11<04:14,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 84%|████████▎ | 378/452 [23:16<04:41,  3.80s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 84%|████████▍ | 379/452 [23:20<04:32,  3.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 84%|████████▍ | 380/452 [23:22<04:04,  3.40s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 84%|████████▍ | 381/452 [23:25<03:42,  3.13s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 85%|████████▍ | 382/452 [23:30<04:22,  3.76s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 85%|████████▍ | 383/452 [23:33<04:00,  3.48s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 85%|████████▍ | 384/452 [23:36<03:43,  3.29s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 85%|████████▌ | 385/452 [23:40<04:08,  3.70s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 85%|████████▌ | 386/452 [23:43<03:45,  3.42s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 86%|████████▌ | 387/452 [23:46<03:32,  3.27s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 86%|████████▌ | 388/452 [23:49<03:23,  3.18s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 86%|████████▌ | 389/452 [23:52<03:15,  3.11s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 86%|████████▋ | 390/452 [23:57<03:46,  3.65s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 87%|████████▋ | 391/452 [24:01<03:43,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 87%|████████▋ | 392/452 [24:04<03:41,  3.69s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 87%|████████▋ | 393/452 [24:09<03:51,  3.92s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 87%|████████▋ | 394/452 [24:12<03:33,  3.69s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 87%|████████▋ | 395/452 [24:15<03:11,  3.37s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 88%|████████▊ | 396/452 [24:18<03:09,  3.38s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 88%|████████▊ | 397/452 [24:22<03:22,  3.69s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 88%|████████▊ | 398/452 [24:26<03:12,  3.56s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 88%|████████▊ | 399/452 [24:28<02:52,  3.25s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 88%|████████▊ | 400/452 [24:31<02:44,  3.15s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 89%|████████▊ | 401/452 [24:36<03:09,  3.71s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 89%|████████▉ | 402/452 [24:40<03:15,  3.91s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 89%|████████▉ | 403/452 [24:44<03:05,  3.79s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 89%|████████▉ | 404/452 [24:48<02:59,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 90%|████████▉ | 405/452 [24:51<02:52,  3.68s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 90%|████████▉ | 406/452 [24:54<02:44,  3.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 90%|█████████ | 407/452 [24:58<02:44,  3.65s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 90%|█████████ | 408/452 [25:01<02:33,  3.48s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 90%|█████████ | 409/452 [25:04<02:22,  3.32s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 91%|█████████ | 410/452 [25:07<02:13,  3.17s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 91%|█████████ | 411/452 [25:12<02:26,  3.56s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 91%|█████████ | 412/452 [25:16<02:26,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 91%|█████████▏| 413/452 [25:19<02:22,  3.67s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 92%|█████████▏| 414/452 [25:22<02:15,  3.56s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 92%|█████████▏| 415/452 [25:26<02:08,  3.48s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 92%|█████████▏| 416/452 [25:30<02:12,  3.69s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 92%|█████████▏| 417/452 [25:33<02:01,  3.46s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 92%|█████████▏| 418/452 [25:37<02:08,  3.77s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 93%|█████████▎| 419/452 [25:40<01:57,  3.55s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 93%|█████████▎| 420/452 [25:44<01:52,  3.52s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 93%|█████████▎| 421/452 [25:50<02:09,  4.19s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 93%|█████████▎| 422/452 [25:53<01:57,  3.92s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 94%|█████████▎| 423/452 [25:57<01:50,  3.82s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 94%|█████████▍| 424/452 [26:00<01:41,  3.64s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 94%|█████████▍| 425/452 [26:05<01:50,  4.10s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 94%|█████████▍| 426/452 [26:08<01:39,  3.81s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 94%|█████████▍| 427/452 [26:12<01:33,  3.74s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 95%|█████████▍| 428/452 [26:15<01:23,  3.49s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 95%|█████████▍| 429/452 [26:18<01:23,  3.61s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 95%|█████████▌| 430/452 [26:21<01:13,  3.33s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 95%|█████████▌| 431/452 [26:25<01:15,  3.58s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 96%|█████████▌| 432/452 [26:28<01:08,  3.44s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 96%|█████████▌| 433/452 [26:34<01:19,  4.19s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 96%|█████████▌| 434/452 [26:37<01:09,  3.87s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 96%|█████████▌| 435/452 [26:42<01:07,  3.96s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 96%|█████████▋| 436/452 [26:44<00:58,  3.63s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 97%|█████████▋| 437/452 [26:48<00:52,  3.53s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 97%|█████████▋| 438/452 [26:51<00:46,  3.30s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 97%|█████████▋| 439/452 [26:55<00:45,  3.53s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 97%|█████████▋| 440/452 [26:59<00:44,  3.71s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 98%|█████████▊| 441/452 [27:02<00:41,  3.73s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 98%|█████████▊| 442/452 [27:06<00:38,  3.81s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 98%|█████████▊| 443/452 [27:11<00:35,  3.89s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 98%|█████████▊| 444/452 [27:14<00:29,  3.66s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 98%|█████████▊| 445/452 [27:16<00:21,  3.14s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 99%|█████████▊| 446/452 [27:20<00:20,  3.43s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 99%|█████████▉| 447/452 [27:24<00:19,  3.81s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 99%|█████████▉| 448/452 [27:30<00:16,  4.21s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            " 99%|█████████▉| 449/452 [27:32<00:11,  3.71s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|█████████▉| 450/452 [27:38<00:08,  4.31s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|█████████▉| 451/452 [27:42<00:04,  4.24s/it]/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1384: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "100%|██████████| 452/452 [27:45<00:00,  3.69s/it]\n"
          ]
        }
      ],
      "source": [
        "answers=[]\n",
        "from tqdm import tqdm\n",
        "for i in tqdm(range(1000,1452)):\n",
        "  question=clean_df['question'].iloc[i]\n",
        "  option1=clean_df['answers'].iloc[i][0]['answer']\n",
        "  option2=clean_df['answers'].iloc[i][1]['answer']\n",
        "  option3=clean_df['answers'].iloc[i][2]['answer']\n",
        "  option4=clean_df['answers'].iloc[i][3]['answer']\n",
        "  # question=\"For the question\"+ question +\"choose the correct answer from the following answers\" + option1 +\",\"+ option2 +\",\"+ option3 + \",\"+option4\n",
        "  question=\"For the question: \"+ question +\" ,choose the correct answer from the following answers: option1) \" + option1 +\", option2) \"+ option2 +\", option3) \"+ option3 + \", option4) \"+option4\n",
        "  response = qa({\"query\": question},return_only_outputs=True)\n",
        "  #print(response['result'])\n",
        "  answers.append(response['result'].lower().replace(clean_df['question'].iloc[i].lower(),'').strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "3jChJI1hJYjA",
      "metadata": {
        "id": "3jChJI1hJYjA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Step 1: Create an empty DataFrame\n",
        "columns = ['predicted_answer', 'groundtruth', 'matches?','options']  # Replace with your column names\n",
        "result_df = pd.DataFrame(columns=columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mC2-KFGcJZWQ",
      "metadata": {
        "id": "mC2-KFGcJZWQ"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "cnt=0\n",
        "j=0\n",
        "for i in range(len(answers)):\n",
        "  matches=0\n",
        "  if answers[j].lower().strip().translate(str.maketrans('', '', string.punctuation)).replace('.','').replace('helpful','').replace('help','')==clean_df.tail(452)['correct_answer'].values[i].lower().strip().translate(str.maketrans('', '', string.punctuation)).replace('.',''):\n",
        "    cnt+=1\n",
        "    matches=1\n",
        "\n",
        "  row_data = {'predicted_answer': answers[j], 'groundtruth': clean_df.tail(452)['correct_answer'].values[i], 'matches?': matches, 'options':clean_df.tail(452)['answers'].values[i]}  # Replace with your values\n",
        "  result_df = result_df.append(row_data, ignore_index=True)\n",
        "  j+=1\n",
        "print(str(cnt)+\" out of \" + str(len(answers))+\" are correct\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6PfK5f7JKs47",
      "metadata": {
        "id": "6PfK5f7JKs47"
      },
      "outputs": [],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "irPfsmPlM0-W",
      "metadata": {
        "id": "irPfsmPlM0-W"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "def rouge_l_score(reference, generated):\n",
        "    smoothing_function = SmoothingFunction().method1  # Choose a smoothing function\n",
        "\n",
        "    # Tokenize the strings into lists of words\n",
        "    reference_tokens = reference.split()\n",
        "    generated_tokens = generated.split()\n",
        "\n",
        "    # Compute ROUGE-L score\n",
        "    rouge_l = sentence_bleu([reference_tokens], generated_tokens, smoothing_function=smoothing_function)\n",
        "\n",
        "    return rouge_l\n",
        "\n",
        "\n",
        "def map_to_option_id_gd(row):\n",
        "  target_answer=row['groundtruth']\n",
        "  answer_options=row['options']\n",
        "  for option in answer_options:\n",
        "      if target_answer.lower() in option['answer'].lower():\n",
        "          return int(option['id'])\n",
        "  return None\n",
        "def map_to_option_id_pred(row):\n",
        "  target_answer=row['predicted_answer']\n",
        "  answer_options=row['options']\n",
        "  maxrouge=0\n",
        "  id=None\n",
        "  for option in answer_options:\n",
        "      rouge_lscore=rouge_l_score(option['answer'].lower(),target_answer.lower())\n",
        "      if rouge_lscore > maxrouge:\n",
        "          maxrouge=rouge_lscore\n",
        "          id=int(option['id'])\n",
        "  return id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "s-AZfCm6M2J9",
      "metadata": {
        "id": "s-AZfCm6M2J9"
      },
      "outputs": [],
      "source": [
        "result_df['result_id_groundtruth'] = result_df.apply(map_to_option_id_gd, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "r29noplFOBYB",
      "metadata": {
        "id": "r29noplFOBYB"
      },
      "outputs": [],
      "source": [
        "result_df['result_id_prediction']=result_df.apply(map_to_option_id_pred, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "036Lf1WAOODu",
      "metadata": {
        "id": "036Lf1WAOODu"
      },
      "outputs": [],
      "source": [
        "result_df['matches_ids']=result_df.result_id_groundtruth==result_df.result_id_prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "POQjgqeuReAb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POQjgqeuReAb",
        "outputId": "1c07857b-e61a-4048-902a-8edbf88f76c3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.40707964601769914"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df.matches_ids.value_counts()[True]/len(result_df)  #accuracy of google flan base model after finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "eNeCt-gOOPLw",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNeCt-gOOPLw",
        "outputId": "43b7e566-87da-43ba-a882-f4685caa5bef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "False    268\n",
              "True     184\n",
              "Name: matches_ids, dtype: int64"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df.matches_ids.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "nnrPjGwGZTUt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nnrPjGwGZTUt",
        "outputId": "abbc65a3-c0fd-410f-ea49-e0909c99a8a1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-9b54012f-9544-414a-947a-5cb6154586ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>predicted_answer</th>\n",
              "      <th>groundtruth</th>\n",
              "      <th>matches?</th>\n",
              "      <th>options</th>\n",
              "      <th>result_id_groundtruth</th>\n",
              "      <th>result_id_prediction</th>\n",
              "      <th>matches_ids</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>valley inversion.</td>\n",
              "      <td>Valley inversion.</td>\n",
              "      <td>1</td>\n",
              "      <td>[{'id': 1, 'answer': 'Valley inversion.'}, {'i...</td>\n",
              "      <td>1</td>\n",
              "      <td>1.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>increases exponentially.</td>\n",
              "      <td>Increases exponentially.</td>\n",
              "      <td>1</td>\n",
              "      <td>[{'id': 1, 'answer': 'Increases linearly betwe...</td>\n",
              "      <td>2</td>\n",
              "      <td>2.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>surface friction.</td>\n",
              "      <td>Horizontal pressure difference.</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 1, 'answer': 'Earth rotation.'}, {'id'...</td>\n",
              "      <td>2</td>\n",
              "      <td>3.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11 to 50 km.</td>\n",
              "      <td>11 to 50 km.</td>\n",
              "      <td>1</td>\n",
              "      <td>[{'id': 1, 'answer': '50 to 85 km.'}, {'id': 2...</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>snow</td>\n",
              "      <td>Hail</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 1, 'answer': 'Hail'}, {'id': 2, 'answe...</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>447</th>\n",
              "      <td>cloud base lower than 2 000 ft and colder air ...</td>\n",
              "      <td>Cloud base higher than 2 000 ft and colder air...</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 1, 'answer': 'Cloud base higher than 2...</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>448</th>\n",
              "      <td>cumulonimbus</td>\n",
              "      <td>Altocumulus</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 1, 'answer': 'Altocumulus'}, {'id': 2,...</td>\n",
              "      <td>1</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>449</th>\n",
              "      <td>maritime warm air flows over a relatively warm...</td>\n",
              "      <td>a mild moist airstream flows over snow covered...</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 1, 'answer': 'maritime warm air flows ...</td>\n",
              "      <td>2</td>\n",
              "      <td>1.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>ice crystal icing on the leading edges due to ...</td>\n",
              "      <td>Frost forming around the fuel tanks due to col...</td>\n",
              "      <td>0</td>\n",
              "      <td>[{'id': 1, 'answer': 'Mixed ice on the leading...</td>\n",
              "      <td>3</td>\n",
              "      <td>2.0</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>-4°c</td>\n",
              "      <td>-4°C</td>\n",
              "      <td>1</td>\n",
              "      <td>[{'id': 1, 'answer': '0°C'}, {'id': 2, 'answer...</td>\n",
              "      <td>3</td>\n",
              "      <td>3.0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>452 rows × 7 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9b54012f-9544-414a-947a-5cb6154586ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9b54012f-9544-414a-947a-5cb6154586ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9b54012f-9544-414a-947a-5cb6154586ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-eed9d17c-cbc3-4e92-bfd0-16d814f4bc0a\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eed9d17c-cbc3-4e92-bfd0-16d814f4bc0a')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-eed9d17c-cbc3-4e92-bfd0-16d814f4bc0a button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                      predicted_answer  \\\n",
              "0                                    valley inversion.   \n",
              "1                             increases exponentially.   \n",
              "2                                    surface friction.   \n",
              "3                                         11 to 50 km.   \n",
              "4                                                 snow   \n",
              "..                                                 ...   \n",
              "447  cloud base lower than 2 000 ft and colder air ...   \n",
              "448                                       cumulonimbus   \n",
              "449  maritime warm air flows over a relatively warm...   \n",
              "450  ice crystal icing on the leading edges due to ...   \n",
              "451                                               -4°c   \n",
              "\n",
              "                                           groundtruth matches?  \\\n",
              "0                                    Valley inversion.        1   \n",
              "1                             Increases exponentially.        1   \n",
              "2                      Horizontal pressure difference.        0   \n",
              "3                                         11 to 50 km.        1   \n",
              "4                                                 Hail        0   \n",
              "..                                                 ...      ...   \n",
              "447  Cloud base higher than 2 000 ft and colder air...        0   \n",
              "448                                        Altocumulus        0   \n",
              "449  a mild moist airstream flows over snow covered...        0   \n",
              "450  Frost forming around the fuel tanks due to col...        0   \n",
              "451                                               -4°C        1   \n",
              "\n",
              "                                               options  result_id_groundtruth  \\\n",
              "0    [{'id': 1, 'answer': 'Valley inversion.'}, {'i...                      1   \n",
              "1    [{'id': 1, 'answer': 'Increases linearly betwe...                      2   \n",
              "2    [{'id': 1, 'answer': 'Earth rotation.'}, {'id'...                      2   \n",
              "3    [{'id': 1, 'answer': '50 to 85 km.'}, {'id': 2...                      3   \n",
              "4    [{'id': 1, 'answer': 'Hail'}, {'id': 2, 'answe...                      1   \n",
              "..                                                 ...                    ...   \n",
              "447  [{'id': 1, 'answer': 'Cloud base higher than 2...                      1   \n",
              "448  [{'id': 1, 'answer': 'Altocumulus'}, {'id': 2,...                      1   \n",
              "449  [{'id': 1, 'answer': 'maritime warm air flows ...                      2   \n",
              "450  [{'id': 1, 'answer': 'Mixed ice on the leading...                      3   \n",
              "451  [{'id': 1, 'answer': '0°C'}, {'id': 2, 'answer...                      3   \n",
              "\n",
              "     result_id_prediction  matches_ids  \n",
              "0                     1.0         True  \n",
              "1                     2.0         True  \n",
              "2                     3.0        False  \n",
              "3                     3.0         True  \n",
              "4                     2.0        False  \n",
              "..                    ...          ...  \n",
              "447                   2.0        False  \n",
              "448                   2.0        False  \n",
              "449                   1.0        False  \n",
              "450                   2.0        False  \n",
              "451                   3.0         True  \n",
              "\n",
              "[452 rows x 7 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "result_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hl25ah5IZVKO",
      "metadata": {
        "id": "Hl25ah5IZVKO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6505a3b",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1fd327e2",
      "metadata": {},
      "source": [
        "# LLM Fine tuning results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1130ba6a",
      "metadata": {},
      "source": [
        "\n",
        "| Embedding model | TextSplitter | chunk_size | list_of_documents | LLM model | retriever(top-k) | Accuracy | correct_answers |\n",
        "|----------|----------|----------|----------|----------|----------|----------|----------|\n",
        "| all-MiniLM-l6-v2  | Character | 256   | explanation  + book1 + book2   | google/flan-t5-small   |  10  | 0.33  | 149 |\n",
        "| all-MiniLM-l6-v2  | RecursiveCharacter | 256  | explanation + book1 + book2   | google/flan-t5-base  | 10 | 0.41  | 187  |\n",
        "| all-MiniLM-l6-v2  | RecursiveCharacter  | 256  | explanation + book1 + book2  | google/flan-t5-large  | 10  | 0.46  | 210 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97db9eb2",
      "metadata": {},
      "source": [
        "\n",
        "> num_epoch = 10\n",
        "\n",
        "> learning_rate = 15e-6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "065b9efb",
      "metadata": {},
      "source": [
        "## Refenence:\n",
        "\n",
        "1. https://github.com/huggingface/peft"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d92707",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.11 ('accelerate': conda)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "vscode": {
      "interpreter": {
        "hash": "1219a10c7def3e2ad4f431cfa6f49d569fcc5949850132f23800e792129eefbb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
